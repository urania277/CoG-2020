\subsection{Objectives}
\label{sub:objectives}
\smallskip

The project's overarching aim is to discover or constrain the particle nature of dark matter via its production at the LHC, 
in synergy with the global search for DM. 
To reach this aim, we will implement data-taking techniques that enable the ATLAS experiment greatly increase its physics output from the upcoming LHC dataset without requiring a significant increase in resources. 
Physics results and software tools resulting from this proposal will be shared, in line with Open Science and FAIR principles~\cite{FAIR}.
The objectives of each WP in this proposal are described in the following sections. 
%, while the impact and associated risks and mitigation strategies are described in Sec.~\ref{sec:RiskImpact}.  

% and their interconnections are displayed in \textbf{Fig.~\ref{fig:WPs}}. 

\subsubsection{Objectives of WP1: advancing real-time analysis in ATLAS}

The first objective of WP1 is to \textbf{deploy a comprensive TLA stream in ATLAS within the new multithreaded ATLAS HLT software in Run-3}. %mentioned above?
This will allow offline-quality photons, muons and electrons to be used for trigger-level physics analysis, in addition to jets. 
The success of the jet prototype has been demonstrated in Run-2 within my StG. 
The use of a jet TLA allowed to lower the HLT jet threshold from 420 GeV in traditional analysis to 220 GeV in TLA, bringing orders-of-magnitude improvements in the number of recorded events. 
%Impact for photons
A TLA implementation of HLT photons will bring significant improvements to the sensitivity of the dijet+ISR DM mediator search. 
The HLT analyzes all photon and electron candidates that have a $p_{\rm{T}}$ above 30 GeV, 
while only single-photon events with a $p_{\rm{T}}$ above 150 GeV are retained in traditional analysis. 
%The work done and lessons learned in the case of trigger-level jets will be the stepping stone for adding photons to the TLA stream, 
%and doubling the signal acceptance for the dijet+ISR search. 
The implementation of electrons and muons in TLA will follow that of photons.
% but they will not be a priority for first data as the difference in thresholds between online and offline events is limited. 
While electron and muon TLAs can still bring significant improvement to e.g. dark photon searches below the Z peak~\cite{Hoenig:2014dsa,Sirunyan:2019wqq}, %CMSDimuon
the primary goal of that work is to gain experience in the reconstruction and calibration of physics objects with constrained computing resources. 
%Secondary impact for the entire collaboration

The second objective of WP1 is to \textbf{implement and reconstruct a data stream combining physics objects reconstructed at the trigger level and selected raw information in restricted regions of the detector}, through the combination of the TLA and Partial Event Building (PEB) techniques. 
Deployed for the first time in ATLAS in this project, this combination will maintain a sufficiently small data format while allowing high-quality physics analysis equaivalent to traditional techniques. 
It will be a necessary ingredient to enable searches in WP4, and can be used for the characterization of any excesses observed in TLA searches. 
%The overall outcome of this work will be a technical publication detailing the early Run-3 trigger.
PEB allows multiple working points depending on the application and detector subset desired, from the factor of $\approx200$ smaller format used for the Run-2 trigger-level analysis of dijets, to factors of $\approx 2-4$ for full-detector information around several prominent objects.
%$B-$physics PEB %cite muon paper if available place the size of TLA+PEB events to less than half of a standard event. \\
\indent
To remain advantageous, TLA and PEB must have the minimal possible footprint in terms of both storage and computing power. 
Joint work betwen WP1 and WP2-4 will ensure that these constraints are met with dedicated trigger studies and selections.   
The budget in this proposal also includes storage servers that will be used to store these data for analysis. 

\begin{wrapfigure}{L}{0.3\textwidth} 
\begin{center}
\includegraphics[width=0.28\textwidth]{figs_B2/efficiencySketch}
\caption{\color{black}\label{fig:wastedRate} \small Sketch of how trigger inefficiencies caused by a mismatch between the HLT and offline jet energy scales causes events to be recorded but never used for analysis.} %Trigger operation page
\end{center}
\vskip-5pt
\end{wrapfigure}

The third objective of WP1 is to \textbf{develop the calibration techniques that are necessary to enable physics analysis} with a reduced HLT-level data format, achieving near-parity with offline performance for the objects needed. 
With my StG team, I have been responsible for reaching a 1-permille agreement between the jet energy scale of HLT jets with respect to offline jets in Run-2~\cite{Aaboud:2018fzt}.%TLAPRL 
Within \textsc{Realdark} we will ensure that this remains the case for jets (as our main observables for WP3), extend it to other physics objects. 
%\color{red}\textbf{Not sure the following should be here, but it's a good motivation to calibrate HLT objects in case someone isn't convinced. }\color{black} 
Differences in the energy scale of HLT and offline objects also lead to inefficiencies in the trigger selection, as events that should pass the trigger are rejected due to HLT miscalibrations. 
As an example, the minimum $p_{\rm{T}}$ threshold applied to jets used in physics analysis is set to be higher than the HLT threshold due to these differences, leading to a substantial waste in terms of events that are recorded but not used for analysis, as shown in Fig.~\ref{fig:wastedRate}. 
Preliminary studies performed within my StG and with collaborators show that, due to the steeply falling jet energy spectrum, the amount of events that are recorded but never used by offline analysis can be as high as 60\% for certain trigger chains. 
The application of TLA-motivated improved calibration constants at the HLT during e.g. LHC technical stops (allowing sufficient time for the reoptimization of the trigger menu) will improve the fraction of useful data recorded and impact the overall ATLAS data-taking. 
%In WP1, my team and I will contribute to the simultaneous derivation of calibration constants for trigger and offline jets. 

The fourth objective of WP1 is to \textbf{further reduce the storage load of TLA events (and more generally ATLAS data and simulation) by compressing this data}. 
TLA data is ideally suited for studies of more aggressive (lossy) compression, since it has already been shown that is robust against reduced amounts of information using dedicated calibrations. If compression and decompression algorithms are sufficiently fast (order of milliseconds) they do not significantly increase the amount of resources needed for data processing. 
The use of machine learning techniques for fast and performant compression (e.g. of images) is widespread: inspired by this, my ATLAS collaborators and I have been supervising Lund University Masterâ€™s students in preliminary tests to compress TLA data using deep autoencoders~\cite{Vincent:2008:ECR:1390156.1390294,hinton}. %cite autoencoders paper 
When compressing 2017 TLA jet data, these studies demonstrated that a compression of factor better than 2 can be achieved with a negligible performance loss.  
In this project, we plan to continue this work and deploy this compression algorithm in a proof-of-principle yet realistic emulation of the trigger system using raw detector data. This is a future-looking study targeting HL-LHC, but if the results are ready to be deployed during the course of Run-3 we will use them for compression of the data recorded with techniques in WP1. 
%Is this also ok with TLA+PEB? 

As the trigger system is crucial for this research program and for the overall ATLAS physics output, the Lund group will maintain a leading role in its operations. 
Throughout the course of this project, the team members will be involved in the development and monitoring of trigger software. 
This is a particularly crucial responsibility during the run up to first data-taking, after the complete overhaul of the ATLAS software. 
The Lund group has already taken responsibility roles in the trigger, with the StG postdoc William Kalderon \footnote{now a postdoctoral fellow at BNL and overall ATLAS trigger menu coordinator.} serving for two years as the convenor of the jet trigger. 

\subsubsection{Objectives of WP2: commissioning the Run-3 trigger system with physics}

The overall objective of WP2 is the commissioning of the new trigger techniques using early LHC data, and their validation using well-established physics observables. 
These observables will be used to evaluate the performance of the calibration techniques and, if needed, implement corrective measures ahead of the production phase of LHC data-taking. 

The first aim of WP2 is the \textbf{determination of the performance of HLT jets and photons, and of electrons and muons} at a later date. 
Together with our collaborators in ATLAS, we will evaluate the energy scale and the uncertainty of HLT objects using early data and simulation. 
I am an expert in those topics, having derived the very first iteration of the energy scale uncertainty~\cite{Doglioni:2011ema} %Jet cross-section and my thesis 
and having supervised a number of students on this topic since. 
Within my StG, we measured the performance of both offline and trigger jets in early Run-2 data using the same metrics and techniques, 
and we intend to do the same in Run-3 for an impact on all ATLAS early measurements and searches.  

We will use \textbf{early data to commission the new Run-3 trigger software} in the HLT dijet mass spectrum. 
My experience at the beginning of Run-2, where my collaborators and I caught a major L1 trigger misconfiguration before it significantly damaged the first 13~TeV searches~\cite{Collaboration:2035503}, showed that this is a mandatory validation step. 
Subsequently, we will repeat this process for PEB muons and electrons, using the dimuon and dielectron mass peaks from the decay of standard candles ($Z$ and $J/\psi$). 

\begin{wrapfigure}{R}{0.5\textwidth} 
\begin{center}
\includegraphics[width=0.48\textwidth]{figs_B2/TLAPublicWinter2019_L1Lumi.pdf}
\caption{\color{black}\label{fig:triggerLowThreshold} \small In Run-2, the underutilization of HLT resources at low LHC instantaneous luminosity allowed my team and collaborators to design TLA L1 triggers with reduced $p_\mathrm{T}$ thresholds~\cite{ATLASTrigger}.} %Trigger operation page
\vskip2pt
\end{center}
\end{wrapfigure}
%\vskip5pt

If, as expected, the sensitivity of mediator searches in the dijet mass spectrum improves with respect to Run-2 results, we will publish dijet and dijet+ISR searches using early data.
As an example, if the LHC delivers more than $\approx$ 18/fb of low-luminosity runs, dijet searches can profit from much lower jet trigger thresholds as shown in Fig.~\ref{fig:triggerLowThreshold} and surpass sensitivity with respect to current searches\footnote{The analysis of the low-threshold Run-2 data is undergoing as part of my VR project grant, see Funding ID in Part B1.}. 

A second aim of WP2 is \textbf{reproducible end-to-end analysis software for dijet and dijet+ISR DM mediator searches}. 
We will use the RECAST framework to package the entire software stack used for early data analyses so that many of the steps can be executed automatically rather than manually.
Using RECAST at this early stage has three advantages. 
First, it makes intermediate testing and monitoring much faster, as this code can be executed on new data and compared to already-tested data. 
Second, it shortens the time required to go from calibrated data to final plots and analysis results in the analysis iteration with the full LHC dataset in WP3.
Third, it can be used as a back-end to the \href{http://reanahub.io/#documentation}{REANA} framework. %REANA
so that physicists outside ATLAS can re-use our analyses to test their own signals, as discussed in WP5.  

\subsubsection{Objectives of WP3: Dark matter mediator searches}

The overall objective of WP3 is to use Run-3 LHC data recorded with the TLA technique to search for new resonances such as DM mediators in the dijet mass spectrum.
In this project we will focus on the decays of the mediators to light quarks, but the work in WP1 also enables extending the searches to mediators decaying preferentially to heavy quarks. 

The main innovation in this WP is the \textbf{dijet+ISR photon search using the TLA technique}, alongside the ISR gluon signature. 
The two channels are complementary: the ISR jet channel is more sensitive due to higher signal rates, but selecting events using ISR photon can reach lower mediator masses. 
Both channels will be much more sensitive than Run-2 searches done without a TLA, since the threshold on the associated object is lowered from $\approx$ 400 GeV to $\approx$ 220 GeV and from $\approx$ 150 GeV to $\approx$ 40 GeV for jet and photon cases respectively.  
In turn, this increases the signal acceptance by more than one order of magnitude for a mediator mass of 250 GeV~\footnote{The background also increases, but since it is estimated using data-driven techniques as explained in Sec.~\ref{sub:CommonMethodsAnalysisTools} its increase can be managed without a significant loss in sensitivity.}, 
and it lowers the minimum mediator mass to which these searches are sensitive, as shown e.g. in Ref.~\cite{Sirunyan:2019pnb}.%CMS ISR TLA 
We will also maintain our involvement in the \textbf{full Run-2 + Run-3 dataset dijet TLA} towards a legacy TLA publication covering both LHC runs.

\subsubsection{Objectives of WP4: Dark sector searches}

%CD: I have the feeling this is a combination of objectives and methods
The objective of WP4 is to use Run-3 data recorded with the TLA+PEB technique for dark QCD searches. 
\\
A first goal preliminary to data analysis is to \textbf{identify the most promising parameter space for the searches in WP4}. 
We will discuss with the theory and broader DM community, especially with experts in Lund, Heidelberg and Aachen. 
Concretely, this will take place through co-supervision of PhD students with Lund theorists, and dedicated workshops in Lund similar to the \href{https://indico.cern.ch/event/863636/}{Dark Dijets workshop} hosted in Lund in November 2019.
Concretely, this will take place through shared supervision of PhD students and dedicated workshopsThese workshops will allow cross-talk with the community interested in comparing and contrasting regular and dark QCD, leading e.g. to concrete improvements for MC generation. 
As part of this work, we will reinterpret LHC measurements and searches in terms of the benchmark models targeted. 
\\
Another goal of WP4 will be to develop \textbf{techniques for QCD backtround rejection at the HLT}, to allow recording higher signal rates for TLA+PEB events. 
\\
The \textbf{search for semi-visible jets} will be performed first, adapting calibration and performance studies in WP1 and WP2 and analysis techniques in WP3 to this signature. 
Subsequently, we will augment the standard reconstruction techniques in WP2 to correctly identify anomalous content (e.g. leptons in jets) needed for the \textbf{composite jet search}, and search for this signature in the PEB+TLA data stream.  
The data stream used for these searches also enables searches for a wide variety of non-standard jet topologies (e.g. photon-jets~\cite{Ellis:2012sd}, %PhotonJets
jets containing long-lived particles as discussed in Sec.~\ref{sub:stateOfTheArtTheory}) %Long lived jets
at a later date. 
This will have an impact especially after this project ends, when data already taken will be analyzed in further detail during the long LHC shutdown before the HL-LHC. 

\subsubsection{Objectives of WP5: Dissemination, communication, synergies}

The overall objective of WP5 is to connect the results from WP1--4 with the global search for DM to maximize their impact. 
This WP naturally includes the contextualization, communication and dissemination of results, amd tools that can enhance the physics potential of experiments and of DM discoveries or constraints. WP5 covers my work within synergistic initiatives involving LHC experiments and the broader DM search communities that directly benefit the objectives of WP3 and WP4, started with the Dark Matter Forum and in the context of the update of the European Strategy of Particle Physics. Collectively furthering the global understanding of the theoretical and experimental landscape for DM models will sharpen the search targets and make discoveries more likely. WP5 spans the entire course of this project, and has the following goals. 
\\
Firstly, I will \textbf{continue co-organizing the Initiative for DM in Europe and beyond (\href{https://indico.cern.ch/e/iDMEu/}{iDMEu})}, a cross-community effort focused on DM that includes the nuclear physics, astroparticle physics and particle physics communities. 
This initiative has more than 200 endorsers at the time of writing, and will have its first kick-off meeting in early Summer 2020. 
It has the ambition of becoming a permanent forum so that all different communities can identify opportunities to work together and exploit synergies and complementarities. 
Practical outcomes of work within this initiative are summary plots with commonly used benchmarks that enhance the complementarity of the LHC results in WP3 and WP4 with other experiments and astrophysical observations. iDMEu also includes a component of communication to the general public, also part of WP5. 
\\
Secondly, we will \textbf{disseminate the technical outcomes of WP1 to other experiments at the LHC and beyond}. 
This will be achieved via technical peer-reviewed papers that include links to prototype open source software implementations as auxiliary materials and presentation in conferences. 
I have recently become part of the coordination team of the cross-experiment \href{https://hepsoftwarefoundation.org}{HEP Software Foundation} (HSF). 
With its goal of helping experiments meeting the challenges posed by new experimental programmes for HL-LHC, the HSF is an ideal platform to connect the solutions in this project to experimental needs. 
%This goal also includes concrete work on data compression with the EGO/Virgo experiment. 
\\
Thirdly, we will \textbf{make results and data from WP3 and WP4 as accessible as possible}, both inside anTod outside the ATLAS Collaboration. 
We will publish the final analysis likelihoods from WP2-4 and implement the end-to-end RECAST analyses in the \href{http://reanahub.io/#documentation}{REANA} hub, %REANA
so that they can be scrutinized and reproduced without a need to directly access ATLAS data (which may only be available in open format at a later date).
These analyses will become part of a broader effort the cross-experiment \textit{Virtual DM environment} for end-to-end data analysis within the European Science Cluster of Astronomy and Particle Physics ESFRI research infrastructure (ESCAPE), which I start leading at the beginning of 2020. 
All software implementations within this project will become part of the \href{https://projectescape.eu/services/escape-software-data-catalogue}{ESCAPE Software Catalogue}. 


