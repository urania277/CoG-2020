
 the establishing  


To further exploit synergies in the global quest for DM, I will establish and co-lead a common platform for and results that has the ambition of involving  communities%. 

WP5 aims to establish a common platform for dissemination and exploitation 

USED:
The well-established complementarity of DD, ID and LHC experiments, acknowledged since the early 2000s~\cite{Snowmass} and fully rooted in the LHC DM search program established during the course of my StG~\cite{DMWG}, only paints a partial picture of the thriving landscape that is the DM community. 
Depending on the SM-DM interaction strength as well as on the DM mass, a wealth of new experiments contribute to the quest for WIMP and non-WIMP DM. These range from accelerator-based to table-top experiments~\cite{PBC}, to experiments sensitive to gravitational wave signals~\cite{BertoneGW,Cirelli}.  
It is clear that, given the breadth of possible explanations for DM, an equally broad approach that involves all possible detection techniques is needed.

These results will be a further step in my career as an established leader in the DM community, enhancing my contributions to the exploitation of synergies and complementarities among the whole experimental and theoretical DM community. 

TO BE ADDED in METHODS
I have considerable experience in this kind of searches, having been one of the lead authors and paper editor of the two flagship iterations of this analysis published immediately after the increase in LHC center of mass energies in 2012 (7 $\rightarrow$ 8 TeV) and 2015 (8 $\rightarrow$ 13 TeV). 


%Throughout 
%\textbf{Mention RECAST!!!! Also the fact that it is only used for reinterpretation and it could do more. WP5.} 

%focus on the less-covered promptly decaying dark jet searches, and on the case where leptons and hadronic particles are found within the same jet. 
%Given this experimental landscape, the searches in WP4 will cover the semi-visible jet signature and the composite jet signature where the jet contains a significant amount of leptons, at medium and high energies. 
%Searches that exploit the long lifetime of single dark sector particles, while models with promptly-decaying dark sector particles are less covered

TO BE ADDED IN OBJECTIVES?
The dissemination of these results and of the tools used to make them possible, in conference talks, peer-reviewed papers, reinterpretation material and software, will have a lasting impact in the search for DM. 







%Move to experimental status

\textbf{EXTRA TEXT FROM HERE ONWARDS}


\subsubsection{Synergies}


%\subsection{Theoretical framework}
%\smallskip


%Generality caveat to be added: from Strassler's talk at 3rd LLP community workshop
%Jets, Suep, and Who Knows What Else 
%• QFT is known to exhibit many different phenomena
%• This means that there is enormous diversity in the phenomenology of hidden sectors at LHC
%• Especially since we know so little about what hidden sectors to look for
%• Any even within a model, small changes can drastically change the pheno
%• In the majority of theories, we cannot calculate what will happen
%• At best we can learn a few facts that will help guide our searches
%• There are surely some phenomena that we don’t yet suspect
%• There is no hope of a MC for most non-perturbative hidden sector theories
%• Therefore: we must search for LLPs in general ways, not reliant on
%• QCD-likehiddensectordynamics
%• UnreliableandunvalidatedMCs
%• Specific corners where QFT dynamics is known in an extreme limit
%• Unless all the potential implications have been worked out by theorists


\subsection{The state of the art in experimental tools}
\smallskip




%Searches for semi-visible jets are only being prototyped at the end of Run-2. No public results exist to date, and the current search targets use assumptions that prevent the model from being compatible with the current observations of the DM relic density assuming thermal freeze-out. %Check Cohen
%A concrete model that include a DM candidate that can satisfy the relic density observation for a wide range of model parameters can be found in ~\cite{Felix, Cohen}. This model includes a $Z’$ mediator, so that searches in WP3 can already inform the most promising parameter space, but it is already clear in~\cite{Felix} that the region with a  $Z’$ below the GeV is the most challenging, and one that can be targeted with the combination of these new methods in this proposal. 

%Jets whose content includes the leptonic decay products of dark photons interleaved in the hadronization of dark quarks are not yet covered by any LHC searches. 
%\textbf{[write more about this, after reading lepton-jets and/or deciding whether it is better than I do more photon-jets]}

%WIMP

%As mentioned in Sec.~\ref{sec:StateOfTheArt_DM}, DM particles generated from the decay of mediators appear in ATLAS as missing transverse momentum (MET) recoiling against e.g. other particles radiated in the initial state. 
%Generic “MET+X” searches are a cornerstone of the LHC DM program, and are currently limited by systematic uncertainties in experimental techniques and theoretical background estimations.  %so what???
%non-WIMP




%Better for methods
%Searches that constrain the mass range of individual dark mesons (e.g.~\cite{DarkMesonSearches}) are not yet included in the survey above. 
%Mention that it doesn’t matter much because we scan? 
%Connecting search for dark jets and searches for their individual components is part of WP4 in this proposal, in order to better tailor parameter scans for concrete benchmark models to be tested in ATLAS searches. 
 


%More shitsalad about theories below!
%\section{State of the art and knowledge gap filled by this proposal}

%One of the most striking gaps in the knowledge of our universe is the nature of dark matter. All we know about dark matter so far comes from its gravitational and astrophysics observations and simulations; astrophysics also provides some tantalizing hints towards the existence of a new particle~\cite{HooperLeane} that are imperative to pursue alongside alternative theories~\footnote{Caveat about gravitation}. 

%In the past decade, the presence of a new massive subatomic particle that interacts weakly with ordinary particles (weakly interacting massive particles, or WIMPs) has been the dominant paradigm to explain the particle nature of dark matter. These particles are postulated by many theories (e.g. supersymmetry) and could be observed at a variety of experiments and at the LHC. Part of the appeal of such theories stems from their ability to explain the entirety of the relic density of dark matter in a simple way~\cite{WIMPMiracle}. The search for WIMPs is far from over: their interactions with ordinary matter could be more rare and not yet accessible by current searches, and both collider and astrophysical searches plan to continue probing these models with their upgrades into the next decade and beyond~\cite{Astro2020, EuropeanStrategy}. 


%A new community of experiments looking for non-WIMP scenarios has flourished~\cite{PBC} and recent breakthroughs in multimessenger astronomy are contributing to defining the landscape of DM searches~\cite{GWBertone}. 
%It is the perfect opportunity for LHC experiments to complement those searches, and to connect results for a broad picture of more complex DM scenarios that will be necessary for a coherent picture of a breakthrough discovery. 

%These are strong motivations for this project to further searches for WIMPs to unprecedented precision, and enable detection of different DM candidates with broad theoretical connections, such as new particles from dark/hidden sectors~\cite{HiddenSector}. 


%%%%%%%%%
%%%OBJECTIVES
%%%%%%%%%


%%%%%%%%%
%%%WP1
%%%%%%%%%

\paragraph{Partial Event Building and TLA}


We will start from the Run-2 implementation of jets that I am currently developing 



enabling reconstruction of offline-quality jets, photons, electrons and muons directly at the trigger level. 
 
Since they contain trigger-level objects only, TLA events are much smaller than events used in ordinary analysis. This permits to significantly lower the minimum transverse momentum threshold of events that are retained for further analysis. For jets, the threshold is lowered from 420 GeV in traditional analysis to 220 GeV in TLA. This brings orders-of-magnitude improvements in the number of events that can be recorded using this technique, as shown in Fig.~\ref{fig:TLASpectrum}. %this sentence sucks

The implementation of TLA jets in the jet trigger software is a prerequisite for the searches for hadronically-decaying dark matter mediators in WP2 and WP3. In WP1, my team and I will \textbf{ensure that the TLA jet stream is implemented and monitored correctly within the new multithreaded ATLAS software}. 





In WP1, we will bring significant improvements to the ATLAS trigger system and continue contributing to its management and operations. 
￼
\paragraph{New Trigger Level Analyses in ATLAS}



The dijet+ISR searches in WP3 only use the ISR photons to select the event, while the invariant mass of the DM mediator candidate is reconstructed using jets. Therefore, the performance of trigger-level photons obtained in Run-2~\cite{EgammaTriggerPaper} and preliminary studies performed by a Bachelor’s student at Lund University~\cite{LeoThesis} already give sufficient confidence of the feasibility of an event selection with TLA photons. 
The main challenge for this search lies in distinguishing low-$p_{\rm{T}}$ jets coming from the hard scatter process from pile-up jets, and efficiently subtracting the pile-up contributions from the hard scatter jets. 
The work done in WP1 includes meeting this challenge with new pile-up suppression and subtraction techniques that use both calorimeter information and the newly available tracking information. 

The \textbf{implementation of electrons and muons in TLA} will follow that of photons, but it will not be a priority for first data as the difference in thresholds between online and offline events is only of a few GeV. While this can still bring significant improvement to e.g. dark photon searches below the Z peak~\cite{CMSDimuon}, the team will work on this implementation to understand the challenges of reconstruction and calibration of different physics objects with constrained computing resources towards the searches in WP4 and as a forward-looking activity to demonstrate that more of the ATLAS data analysis program can be moved to the trigger with significant storage gains. 

To remain advantageous, a \textbf{TLA must have the minimal possible footprint in terms of both storage and computing power}. Work in WP1 will ensure that these constraints are met with dedicated algorithms.  

\paragraph{Partial Event Building and TLA}

The second objective of WP1 is to \textbf{implement a data stream combining physics objects reconstructed at the trigger level and full information in restricted regions of the detector}, through the combination of the TLA and Partial Event Building (PEB) techniques. 

Deployed for the first time in ATLAS in this proposal, this combination will maintain a sufficiently small data format with an amount of information equivalent to traditional techniques. It will be a necessary ingredient to enable the non-standard reconstructions needed for the searches in WP4, and enable further characterization of excesses in a TLA search. 

In addition to the \textbf{core software work} required to write out trigger objects and raw data, a TLA+PEB stream also requires \textbf{adapting the detector reconstruction software} so that it can correctly reconstruct objects only within a certain region of interest. 

Preliminary estimates based on the Run-2 muon PEB in ATLAS~\cite{PEBIsThereEvenAPaper} place the size of TLA+PEB events to approximately 20\% of a standard event. This will still require dedicated trigger chains to be developed within WP1. The budget of this proposal also includes storage servers that will be used to store this data in 2023-2024. 


\paragraph{Trigger operations}

As the trigger system is crucial for this research program and for the overall ATLAS physics output, the Lund group will maintain a leading role in its operations. 
Throughout the course of this project, the team members will be involved in the development of trigger software and monitoring of software changes. This is a particularly crucial responsibility during the run up to first data taking, after the complete overhaul of the ATLAS software. 
The Lund group has already taken responsibility roles in the trigger, with the StG postdoc William Kalderon, who has served for two years as the convenor of the jet trigger and has now moved on to be overall ATLAS trigger menu coordinator. 

%%%%%%%%%
%%%WP2
%%%%%%%%%


% for a sensitivity improvement of 20% over the Run-2 dataset alone.

%  129/fb Run2 
%+ 179/fb Run3
%= 300/fb

%Xsec improvement = sqrt(Lumi ratio); 
%Coupling improvement = sqrt(xsec improvement)
%>>> sqrt(129+179)/sqrt(129)
%1.5451850695708933
%>>> sqrt(1.5)
%1.224744871391589

%So if we were sensitive to 0.05 before, now we are sensitive to 0.04. Wow how shit. 



%%%%%%%%%
%%%PROJECT PLANNING - CALIBRATION
%%%%%%%%%


\paragraph{Jet calibration} My team and I have long-standing expertise in the calibration of jets (see e.g. Refs.~\cite{JESPapersIAmAuthorOf} which I have edited or contributed to over the course of my career). 
Jet calibration requires a multi-step procedure as described in Ref.~\ref{sec:JESPaper}, since they are composite objects whose individual constituents undergo a variety of detector effects that can either add or subtract from the “true” jet energy. 

Not all the calibration steps could be performed at the HLT in Run-2 due to the lack of tracking information. As described in Sec.~\ref{sec:StateOfTheArt}, in Run-3 it is possible to associate information about tracks from the primary vertex to HLT jets.  This is a key point for TLAs sensitive to low-mass DM mediators. Currently, jets with a $p_{\rm{T}}$ below 60 GeV are not used in TLA as the contribution from pile-up cannot be removed efficiently using calorimeter information only. Tracking allows much more efficient subtraction of pile-up energy as well as the rejection of pile-up jets, effectively removing the restriction on the use of low-$p_{\rm{T}}$ trigger jets. Tracking information is also used to improve the resolution of trigger jets~\cite{GSC}, which in turn improves the mass resolution for new hadronically-decaying resonances. 
With these improvements, the performance of HLT jets in Run-3 will be significantly improved with respect to Run-2. 

%Don’t care about this really because we always recalibrate
%Calibration constants for jets are derived more than once, with each iteration improving the precision of the jet energy scale. 
%The “ab-initio” calibration constants applied to HLT jets for first data will be derived using MC only. 
 
Calibration constants for jets are derived more than once, with each iteration improving the precision of the jet energy scale. Improved calibrations can be easily applied to both offline and HLT jets prior to their use in analysis. 
Nevertheless, using a less precise jet calibration for HLT jets than for jets used in offline analysis leads to \textit{trigger inefficiencies}, as events that should pass the trigger are rejected due to HLT miscalibration. 
The minimum $p_{\rm{T}}$ threshold applied to jets used in physics analysis is set to correspond to a point where the inefficiency is negligible, and this is generally higher than the HLT threshold. 
This in turn leads to a substantial waste in terms of events that are recorded but not used for analysis, as shown in Fig.~\ref{fig:wastedRate}. 

An improved calibration allows for improved agreement between the scale of trigger and offline jets, with a trigger efficiency that rises much more rapidly. 
For this reason, an ambitious aim of this proposal prior to the LHC production data delivery period is to derive calibration constants simultaneously for trigger and offline jets, relying on the alignment of the reconstruction algorithms and on the availability of the same information for both kinds of jets. 

New calibration constants can then be implemented in the HLT prior to data taking, allowing sufficient time for the reoptimization of the trigger menu taking the new event rates into account. In this way, work that benefit TLA will also have a significant impact on the overall ATLAS data taking strategies. 

\paragraph{Calibration and identification of photons and electrons}
One of the technical innovations of this project is the addition of photons to the TLA stream, so that they can be used for the dijet+ISR photon search in WP2 and WP3.

Photons are not used to construct the main observables for this search, so their identification and calibration is not as critical as for jets. However, as described in the previous section, improving the performance of physics objects at the trigger level benefits the ATLAS physics program as a whole. This work will also enable further analyses to gain from using the TLA technique at a later stage (e.g. searches for low-mass pseudo scalars motivated by Axion-Like Particles~\ref{ALPMariotti}). 

Preliminary studies that I supervised~\ref{Leo Thesis} show that the photon calibration at the trigger level in Run-2 was already well-understood without further optimizations in the energy range used for the searches in WP3. We expect the situation to improve in Run-3, especially since the preliminary studies did not subtract any of the QCD background that can be removed using a combination of calorimeter variables (e.g. as in~\ref{PhotonID,PhotonXSec}). 

In Run-3, the same calorimeter inputs and software will be used to build both HLT and offline photons and electrons, with only minor differences in the calibration of the calorimeter cell energies. The photon calibration at the trigger level is performed in a series of steps that mirrors what is done offline, and the same software will be used to compute data-simulation correction factors~\ref{EgammaTriggerPaper}. The differences in the photon energies due to differences in the calorimeter cell energies will be corrected in this procedure. Control samples of Z bosons decaying to two electrons, one of which radiates a photon, provide a signal-rich testing ground for understanding the performance of HLT photons.  

In this proposal, we plan to investigate whether using tracking for photon at the trigger level would improve their performance. Since jet reconstruction in events used for TLA already requires HLT tracking, the tracks will be available in the event without further CPU costs. Tracking could be used to improve the energy scale of photons that convert into electron-positron pairs in the detector material, and to further reduce hadronic and pile-up backgrounds. Prior to data taking, we will study the impact of improved isolation in the search in the preliminary sensitivity studies (Sec.~\ref{sec:Sensitivity}) and decide whether to make tracking in photons a priority. 

Electrons and photons share most of the reconstruction chain in offline and trigger, with the difference that electrons require an associated tracks and use further information from the inner detector to improve their identification. The HLT electron performance will improve in Run-3, as most of the calibration steps and software are planned to be the same as for offline electrons, and residual differences will be removed using the ratio of HLT to offline electron variables in high-statistics control samples of J/psi and Z bosons decaying to two electrons. 
In this proposal, we will study the performance of HLT electrons (see Sec.~\ref{sec:Performance}), and extend the photon tracking studies to electrons as well.
% do we want to say what we will do?
% with a particular focus in improving the CPU costs of the algorithms used.   

Studies of the reconstruction, calibration and performance of HLT electrons in this proposal have two purposes. 
Firstly, HLT electrons with offline-like quality can be used for searches and measurements. For example, a search for leptonic decays of dark photons using TLA electrons below the current HLT threshold of 26 GeV could be combined with an equivalent search in the muon channel, constraining this models the sensitivity beyond current results~\ref{LHCbDarkPhoton}. Furthermore, tests of lepton flavor universality at $p_{\mathrm{T}}$ below 10 GeV are an interesting yet challenging avenue for experimental study. 
Secondly, we will gain experience in the regional reconstruction and calibration of non-isolated electron candidates as done at the HLT, needed for WP4 searches using partial event building. 

\paragraph{Calibration and identification of muons} 

Muon HLT chains are used for a wide variety of purposes and cover a large range of muon transverse momenta, as they are used to trigger low-mass resonances in the B-physics programs as well as high-pT muons for exotic searches. 
The performance of muons at the trigger level is therefore rather well-understood, and CMS has already a trigger-level (scouting) stream in place used for highly sensitive dark photon searches~\ref{CMSScouting}. In this proposal, we will enable the use of HLT muons for physics analysis in ATLAS in a similar way as described for photons and electrons above. We will rely on existing work  by the muon and B-physics trigger groups in ATLAS for the muon reconstruction and calibration, contribute to the evaluation and monitoring of HLT muon performance, and use these results for WP4 searches. 

\paragraph{Electrons and muons in hadronic environments}

In Run-3, the default jet inputs technique will change, and include tracks for the particle flow technique. 
We will extend the PFlow framework to include electron- and muon-finding from components that are already present within the jet. 
We will employ the understanding of calibration of objects to write new algorithms for non-isolated stuff. 
\textbf{Unfinished}
\paragraph{Calibration and identification of photons and electrons}
One of the technical innovations of this project is the addition of photons to the TLA stream, so that they can be used for the dijet+ISR photon search in WP2 and WP3.

Photons are not used to construct the main observables for this search, so their identification and calibration is not as critical as for jets. However, as described in the previous section, improving the performance of physics objects at the trigger level benefits the ATLAS physics program as a whole. This work will also enable further analyses to gain from using the TLA technique at a later stage (e.g. searches for low-mass pseudo scalars motivated by Axion-Like Particles~\ref{ALPMariotti}). 

Preliminary studies that I supervised~\ref{Leo Thesis} show that the photon calibration at the trigger level in Run-2 was already well-understood without further optimizations in the energy range used for the searches in WP3. We expect the situation to improve in Run-3, especially since the preliminary studies did not subtract any of the QCD background that can be removed using a combination of calorimeter variables (e.g. as in~\ref{PhotonID,PhotonXSec}). 

In Run-3, the same calorimeter inputs and software will be used to build both HLT and offline photons and electrons, with only minor differences in the calibration of the calorimeter cell energies. The photon calibration at the trigger level is performed in a series of steps that mirrors what is done offline, and the same software will be used to compute data-simulation correction factors~\ref{EgammaTriggerPaper}. The differences in the photon energies due to differences in the calorimeter cell energies will be corrected in this procedure. Control samples of Z bosons decaying to two electrons, one of which radiates a photon, provide a signal-rich testing ground for understanding the performance of HLT photons.  

In this proposal, we plan to investigate whether using tracking for photon at the trigger level would improve their performance. Since jet reconstruction in events used for TLA already requires HLT tracking, the tracks will be available in the event without further CPU costs. Tracking could be used to improve the energy scale of photons that convert into electron-positron pairs in the detector material, and to further reduce hadronic and pile-up backgrounds. Prior to data taking, we will study the impact of improved isolation in the search in the preliminary sensitivity studies (Sec.~\ref{sec:Sensitivity}) and decide whether to make tracking in photons a priority. 

Electrons and photons share most of the reconstruction chain in offline and trigger, with the difference that electrons require an associated tracks and use further information from the inner detector to improve their identification. The HLT electron performance will improve in Run-3, as most of the calibration steps and software are planned to be the same as for offline electrons, and residual differences will be removed using the ratio of HLT to offline electron variables in high-statistics control samples of J/psi and Z bosons decaying to two electrons. 
In this proposal, we will study the performance of HLT electrons (see Sec.~\ref{sec:Performance}), and extend the photon tracking studies to electrons as well.
% do we want to say what we will do?
% with a particular focus in improving the CPU costs of the algorithms used.   

Studies of the reconstruction, calibration and performance of HLT electrons in this proposal have two purposes. 
Firstly, HLT electrons with offline-like quality can be used for searches and measurements. For example, a search for leptonic decays of dark photons using TLA electrons below the current HLT threshold of 26 GeV could be combined with an equivalent search in the muon channel, constraining this models the sensitivity beyond current results~\ref{LHCbDarkPhoton}. Furthermore, tests of lepton flavor universality at $p_{\mathrm{T}}$ below 10 GeV are an interesting yet challenging avenue for experimental study. 
Secondly, we will gain experience in the regional reconstruction and calibration of non-isolated electron candidates as done at the HLT, needed for WP4 searches using partial event building. 

\paragraph{Calibration and identification of muons} 

Muon HLT chains are used for a wide variety of purposes and cover a large range of muon transverse momenta, as they are used to trigger low-mass resonances in the B-physics programs as well as high-pT muons for exotic searches. 
The performance of muons at the trigger level is therefore rather well-understood, and CMS has already a trigger-level (scouting) stream in place used for highly sensitive dark photon searches~\ref{CMSScouting}. In this proposal, we will enable the use of HLT muons for physics analysis in ATLAS in a similar way as described for photons and electrons above. We will rely on existing work  by the muon and B-physics trigger groups in ATLAS for the muon reconstruction and calibration, contribute to the evaluation and monitoring of HLT muon performance, and use these results for WP4 searches. 

\paragraph{Electrons and muons in hadronic environments}

In Run-3, the default jet inputs technique will change, and include tracks for the particle flow technique. 
We will extend the PFlow framework to include electron- and muon-finding from components that are already present within the jet. 
We will employ the understanding of calibration of objects to write new algorithms for non-isolated stuff. 
\textbf{Unfinished}


%For this reason, an ambitious aim of this proposal prior to the LHC production data delivery period is to derive calibration constants simultaneously for trigger and offline jets, relying on the alignment of the reconstruction algorithms and on the availability of the same information for both kinds of jets. 
%Jet calibration requires a multi-step procedure as described in Ref.~\ref{sec:JESPaper}, since they are composite objects whose individual constituents undergo a variety of detector effects that can either add or subtract from the “true” jet energy. 




%Not all the calibration steps could be performed at the HLT in Run-2 due to the lack of tracking information., in Run-3 it is possible to associate information about tracks from the primary vertex to HLT jets.  This is a key point for TLAs sensitive to low-mass DM mediators. Currently, jets with a $p_{\rm{T}}$ below 60 GeV are not used in TLA as the contribution from pile-up cannot be removed efficiently using calorimeter information only. Tracking allows much more efficient subtraction of pile-up energy as well as the rejection of pile-up jets, effectively removing the restriction on the use of low-$p_{\rm{T}}$ trigger jets. Tracking information is also used to improve the resolution of trigger jets~\cite{GSC}, which in turn improves the mass resolution for new hadronically-decaying resonances. 


%This is a crucial part of physics analysis and of this project as a whole. 


 physics outcomes 

 tolerance of the physics outcomes to these differences, estimated case by case in WP3 and WP4 as 
 
 
Within WP2, we will prepare a comprehensive software toolkit that evaluates relevant figures of merit such as response and resolution for testing of probe physics objects against well-measured references. %references? Or? Bad wording anyway
With this toolkit, we will make quantitative comparisons between physics objects reconstructed in TLA, TLA+PEB and offline, between data and simulation, and between well-understood data from previous iterations of the analysis with data taken with new conditions.  

Given the non-negligible number of physics objects that will be ultimately implemented in the trigger system and monitored, we will use automated problem detection techniques wherever possible. We will adapt algorithms developed in WP1 for a coarse anomaly detection to pinpoint issues to be investigated more carefully. 

After testing the performance toolkit with early LHC data in WP2, we will implement a selection of features in the monitoring system of the experiment, so that 

This toolkit will also be used to aid a consistent estimation of the energy scale and resolution uncertainties for the objects used in the searches in WP3-4, as well as for the data-MC correction factors (scale-factors) needed when comparing data and simulation, as discussed in Sec.~\ref{sec:Systematics}. 


\subsubsection{Common methodologies: Resource optimization}
\label{subsub:Resource optimization}



In the case of TLA, where reconstruction and a near-final calibration of objects need to be performed in the HLT farm, we will estimate the CPU costs of various options using the ATLAS Cost Monitoring framework~\cite{CostMonitoringTim}, prior to data taking in 2021.  
Preliminary estimates are already available for the dijet+ISR photon analysis based on Run-2 software, indicating that this is tolerable, but further optimizations will be possible taking advantage of the rewrite of the trigger software and of the improved tracking and accounting for the early LHC running conditions (e.g. low luminosity runs) that will be known over the course of 2020. Using those new estimates, we will adjust the thresholds of the trigger chains and the level of precision of the algorithms used to maximize the sensitivity of the TLA dijet and dijet+ISR searches, based on the studies in Sec.\ref{sub:SensitivityStudies}. 
 
In the case of TLA+PEB searches, the use of resources will have to be balanced between using computing-expensive algorithms at the HLT to be able to drop raw detector data, and delaying running those algorithms until later but allowing for a larger event size due to storing more detector information. The Cost Monitoring framework and test events storing different levels of detector information, informed by the calibration and reconstruction work as well as by the sensitivity studies, will be evaluated for this optimization. 


~\ref{CWoLA}. 
These studies will profit from my work in the “unsupervised searches” group in the  Darkmachines collective, which is conducting preliminary studies using simulation of DM-related processes. 

%%%%%%%%%
%%%PROJECT PLANNING
%%%%%%%%%



These studies will also give sufficient handles to understand the variability of QCD jets to evaluate search sensitivity and systematic uncertainties.
They will also provide input to a parallel effort funded by my VR Project Grant focused on anomaly detection techniques.  




 with lower thresholds with respect to offline searches, with sufficient raw detector information in the region behind the jets. 
 


Designing an HLT pre-selection that 
Rejecting sufficient QCD background with an HLT pre-selection will 

Preliminary work to understand the performance 





PD2 and PhD2 will focus on dark sector searches, using data recorded with the TLA+PEB technique for dark QCD searches. 

The work in WP1 and WP2 on the reconstruction, calibration and performance of regional detector data will be the stepping stone for the searches for semi-visible jets and composite jets. 
In WP4, we will \textbf{extend the study of isolated object performance to busier hadronic environments}, with a focus on the identification of variables and analysis techniques that can be used to minimize the impact of pile-up and reduce SM backgrounds already at the trigger level \textbf{[15]}. 
Rejecting sufficient QCD background with a pre-selection will allow recording higher rate of TLA+PEB events with lower thresholds with respect to offline searches, with sufficient raw detector information in the region behind the jets. 
These studies will also give sufficient handles to understand the variability of QCD jets to evaluate search sensitivity and systematic uncertainties.
They will also provide input to a parallel effort funded by my VR Project Grant focused on anomaly detection techniques.  
The \textbf{search for semi-visible jets} will be performed first~\textbf{[16]}, after we have reached sufficient understanding of the hadronic jet content. 
Subsequently, we will augment the standard reconstruction techniques to correctly identify anomalous content (e.g. leptons in jets) needed for the \textbf{composite jet search}~\textbf{[17]}.

An additional advantage of the PEB+TLA data stream selected for these searches is that it enables searches for a wide variety of non-standard jet topologies (e.g. photon-jets~\cite{PhotonJets}, jets containing long-lived particles~\cite{PhotonJets}) at a later date. 
This will have an impact especially after the end of this proposal when the long LHC shutdown is foreseen before HL-LHC and data already taken will be analyzed in further detail. 


\subsection{WP5: Dissemination and synergies}
\textbf{Organization of workshops} 
\textbf{Dissemination of results and summary plots} 
\textbf{Sharing of tools} 


%%%%%%BEGIN TO COMPILE UP TO HERE 
\clearpage
\begingroup
    \setboolean{inbibliography}{true}
\bibliographystyle{LHCb}
    \linespread{0.9}\selectfont
\bibliography{researchrefs}
\endgroup
%{\bf Note:} The PI is an author of Refs.~\cite{LHCb-PUB-2014-027,Bourgeois:2018nvk} and all the LHCb collaboration papers, with particular contributions to  Refs.~\cite{Aaij:2014jba,CERN-LHCC-2014-016,Aaij:2013mga,Aaij:2014ywt,LHCb-PUB-2014-040,Aaij:2016kjh,Aaij:2017lff}.
%The PI is acknowledged in Ref.~\cite{Jung:2014jfa}.
\end{document}  
%%%%%%END TO COMPILE UP TO HERE 


%In particular, the performance of trigger-level photons obtained in Run-2~\cite{ToBeCited} %EGamma paper
%and preliminary studies performed by a Lund Bachelor’s student~\cite{ToBeCited}%LeoBachelorThesis
%already give sufficient confidence on the feasibility of a dijet+ISR search, where the resonance is built from the jets and the photon is used as a tag. 
%The main challenge for this search lies in distinguishing low-$p_{\rm{T}}$ hard-scatter jets from pile-up jets, and efficiently subtracting pile-up contributions. 

%Project planning


%While TLA events are already reconstructed at the HLT and only need to be calibrated (see next section), in the case of the TLA+PEB stream we will need to adapt the offline software reconstruction algorithms to be able to cope with receiving partial detector data as input. 

%%%%%%%%%%%%


\subsubsection{Optimization of trigger resources}

Given that a large part of the searches in this proposal occurs at the HLT, a resource constrained environment, an important part of the planning prior to data taking is to optimize the CPU and storage resources needed for each of the searches. After this optimization, we can deploy the dedicated trigger chains to select events that will be analyzed using the TLA or the TLA+PEB techniques. 

In the case of TLA, where reconstruction and a near-final calibration of objects need to be performed in the HLT farm, we will estimate the CPU costs of various options using the ATLAS Cost Monitoring framework~\cite{CostMonitoringTim}, prior to data taking in 2021.  
Preliminary estimates are already available for the dijet+ISR photon analysis based on Run-2 software, indicating that this is expected, but further optimizations will be possible taking advantage of the rewrite of the trigger software and of the improved tracking and accounting for the early LHC running conditions (e.g. low luminosity runs) that will be known over the course of 2020. Using those new estimates, we will adjust the thresholds of the trigger chains and the level of precision of the algorithms used to maximize the sensitivity of the TLA dijet and dijet+ISR searches, based on the studies in Sec.\ref{sub:SensitivityStudies}. 
 

%%%%%


\subsubsection{Data preparation}


In this proposal “data preparation” indicates the series of methods and procedures that are needed to transform the raw data into final distributions. For non-standard analyses such as TLA and TLA+PEB, these procedures differ with respect to traditional analysis, as more of this process needs to be customized and controlled by the analysis team. 

In brief, the steps taken in this process are:

\begin{itemize}

\item Data are recorded by ATLAS using the trigger chains designed in Sec.~\ref{sec:OptimizationOfResources}. This is when real-time reconstruction of the data happens in the HLT computing farm, and its outputs are directly sent to the TLA stream;
\item A small fraction of data are reconstructed and analyzed right away as data taking occurs for monitoring purposes (both centrally and by the analysis team, using the toolkit in Sec.~\ref{sec:Performance})
\item Raw data, TLA and TLA+PEB data in \textit{bytestream} format are stored on tape at the CERN data center (Tier-0). 
Raw detector data is reconstructed shortly after having been taken, requiring most of the Tier-0 computing resources. 
TLA data requires fewer computing resources at this stage, as it already contains reconstructed objects. 
TLA+PEB data is not reconstructed immediately, but rather transferred to local resources where specialized reconstruction algorithms can be ran, or processed when the LHC is not running~\footnote{This is a procedure I was instrumental in defining and using in Run-1 and Run-2, called “delayed stream” in ATLAS or “data parking” in CMS}. 
\item Reconstructed data are processed from \textit{bytestream} format to a user-readable data format, called \textit{AOD}.  

\end{itemize}

From this point on, AOD data are sent to computing centers worldwide where user analysis occurs. User analysis includes:

\begin{itemize}
\item Derivation of the calibration constants and of their uncertainties. This step is generally done centrally by the experiment’s “Combined Performance groups”. We will participate actively to this work that is useful for the entirety of ATLAS, and customize central recommendations wherever needed for the analyses in WP3 and WP4. 
\item Application of updated calibration constants to the data used for analysis and to the simulation of signal and backgrounds.  
\item Production of final distributions of the observables needed for the monitoring of calibration performance and for further data analysis. This step may require various iterations, e.g. when updated calibration constants are available. 
\item Use of final distributions for e.g. background estimation and statistical analysis.
\end{itemize}

The RECAST framework will be used to steer each of the user analysis steps and automatize them as much as possible. This will allow the analysis to be rerun quickly when more data is added, and it will facilitate further reinterpretation as the full analysis can be rerun just passing a different signal sample through the procedure. 

%%%%

\subsubsection{Calibration}


\paragraph{Jet calibration} My team and I have long-standing expertise in the calibration of jets (see e.g. Refs.~\cite{JESPapersIAmAuthorOf} which I have edited or contributed to over the course of my career). 
Jet calibration requires a multi-step procedure as described in Ref.~\ref{sec:JESPaper}, since they are composite objects whose individual constituents undergo a variety of detector effects that can either add or subtract from the “true” jet energy. 

Not all the calibration steps could be performed at the HLT in Run-2 due to the lack of tracking information. As described in Sec.~\ref{sec:StateOfTheArt}, in Run-3 it is possible to associate information about tracks from the primary vertex to HLT jets.  This is a key point for TLAs sensitive to low-mass DM mediators. Currently, jets with a $p_{\rm{T}}$ below 60 GeV are not used in TLA as the contribution from pile-up cannot be removed efficiently using calorimeter information only. Tracking allows much more efficient subtraction of pile-up energy as well as the rejection of pile-up jets, effectively removing the restriction on the use of low-$p_{\rm{T}}$ trigger jets. Tracking information is also used to improve the resolution of trigger jets~\cite{GSC}, which in turn improves the mass resolution for new hadronically-decaying resonances. 
With these improvements, the performance of HLT jets in Run-3 will be significantly improved with respect to Run-2. 

%Don’t care about this really because we always recalibrate
%Calibration constants for jets are derived more than once, with each iteration improving the precision of the jet energy scale. 
%The “ab-initio” calibration constants applied to HLT jets for first data will be derived using MC only. 
 
Calibration constants for jets are derived more than once, with each iteration improving the precision of the jet energy scale. Improved calibrations can be easily applied to both offline and HLT jets prior to their use in analysis. 
Nevertheless, using a less precise jet calibration for HLT jets than for jets used in offline analysis leads to \textit{trigger inefficiencies}, as events that should pass the trigger are rejected due to HLT miscalibration. 
The minimum $p_{\rm{T}}$ threshold applied to jets used in physics analysis is set to correspond to a point where the inefficiency is negligible, and this is generally higher than the HLT threshold. 
This in turn leads to a substantial waste in terms of events that are recorded but not used for analysis, as shown in Fig.~\ref{fig:wastedRate}. 

An improved calibration allows for improved agreement between the scale of trigger and offline jets, with a trigger efficiency that rises much more rapidly. 
For this reason, an ambitious aim of this proposal prior to the LHC production data delivery period is to derive calibration constants simultaneously for trigger and offline jets, relying on the alignment of the reconstruction algorithms and on the availability of the same information for both kinds of jets. 

New calibration constants can then be implemented in the HLT prior to data taking, allowing sufficient time for the reoptimization of the trigger menu taking the new event rates into account. In this way, work that benefit TLA will also have a significant impact on the overall ATLAS data taking strategies. 

\paragraph{Calibration and identification of photons and electrons}
One of the technical innovations of this project is the addition of photons to the TLA stream, so that they can be used for the dijet+ISR photon search in WP2 and WP3.

Photons are not used to construct the main observables for this search, so their identification and calibration is not as critical as for jets. However, as described in the previous section, improving the performance of physics objects at the trigger level benefits the ATLAS physics program as a whole. This work will also enable further analyses to gain from using the TLA technique at a later stage (e.g. searches for low-mass pseudo scalars motivated by Axion-Like Particles~\ref{ALPMariotti}). 

Preliminary studies that I supervised~\ref{Leo Thesis} show that the photon calibration at the trigger level in Run-2 was already well-understood without further optimizations in the energy range used for the searches in WP3. We expect the situation to improve in Run-3, especially since the preliminary studies did not subtract any of the QCD background that can be removed using a combination of calorimeter variables (e.g. as in~\ref{PhotonID,PhotonXSec}). 

In Run-3, the same calorimeter inputs and software will be used to build both HLT and offline photons and electrons, with only minor differences in the calibration of the calorimeter cell energies. The photon calibration at the trigger level is performed in a series of steps that mirrors what is done offline, and the same software will be used to compute data-simulation correction factors~\ref{EgammaTriggerPaper}. The differences in the photon energies due to differences in the calorimeter cell energies will be corrected in this procedure. Control samples of Z bosons decaying to two electrons, one of which radiates a photon, provide a signal-rich testing ground for understanding the performance of HLT photons.  

In this proposal, we plan to investigate whether using tracking for photon at the trigger level would improve their performance. Since jet reconstruction in events used for TLA already requires HLT tracking, the tracks will be available in the event without further CPU costs. Tracking could be used to improve the energy scale of photons that convert into electron-positron pairs in the detector material, and to further reduce hadronic and pile-up backgrounds. Prior to data taking, we will study the impact of improved isolation in the search in the preliminary sensitivity studies (Sec.~\ref{sec:Sensitivity}) and decide whether to make tracking in photons a priority. 

Electrons and photons share most of the reconstruction chain in offline and trigger, with the difference that electrons require an associated tracks and use further information from the inner detector to improve their identification. The HLT electron performance will improve in Run-3, as most of the calibration steps and software are planned to be the same as for offline electrons, and residual differences will be removed using the ratio of HLT to offline electron variables in high-statistics control samples of J/psi and Z bosons decaying to two electrons. 
In this proposal, we will study the performance of HLT electrons (see Sec.~\ref{sec:Performance}), and extend the photon tracking studies to electrons as well.
% do we want to say what we will do?
% with a particular focus in improving the CPU costs of the algorithms used.   

Studies of the reconstruction, calibration and performance of HLT electrons in this proposal have two purposes. 
Firstly, HLT electrons with offline-like quality can be used for searches and measurements. For example, a search for leptonic decays of dark photons using TLA electrons below the current HLT threshold of 26 GeV could be combined with an equivalent search in the muon channel, constraining this models the sensitivity beyond current results~\ref{LHCbDarkPhoton}. Furthermore, tests of lepton flavor universality at $p_{\mathrm{T}}$ below 10 GeV are an interesting yet challenging avenue for experimental study. 
Secondly, we will gain experience in the regional reconstruction and calibration of non-isolated electron candidates as done at the HLT, needed for WP4 searches using partial event building. 

\paragraph{Calibration and identification of muons} 

Muon HLT chains are used for a wide variety of purposes and cover a large range of muon transverse momenta, as they are used to trigger low-mass resonances in the B-physics programs as well as high-pT muons for exotic searches. 
The performance of muons at the trigger level is therefore rather well-understood, and CMS has already a trigger-level (scouting) stream in place used for highly sensitive dark photon searches~\ref{CMSScouting}. In this proposal, we will enable the use of HLT muons for physics analysis in ATLAS in a similar way as described for photons and electrons above. We will rely on existing work  by the muon and B-physics trigger groups in ATLAS for the muon reconstruction and calibration, contribute to the evaluation and monitoring of HLT muon performance, and use these results for WP4 searches. 

\paragraph{Electrons and muons in hadronic environments}

In Run-3, the default jet inputs technique will change, and include tracks for the particle flow technique. 
We will extend the PFlow framework to include electron- and muon-finding from components that are already present within the jet. 
We will employ the understanding of calibration of objects to write new algorithms for non-isolated stuff. 
\textbf{Unfinished}


%%%%


Autoencoders are widely used as anomaly detection algorithms. They are first trained on large amounts of non-anomalous reference data, and then presented with new data. If the new data does not match the reference data, the reconstruction error will be large and can be used as figure of merit to spot anomalies. 

The advantage of autoencoders over methods such as Principal Component Analysis is that they are able to learn non-linear correlations among the nodes, and that once the network is trained the compression is much faster. 
The engineering Master’s student working with me in Fall 2019 compressed 2017 TLA jet data in text file format, and demonstrated that a compression of factor of 2 can be achieved with a negligible performance loss (measured to be within the experimental resolution) and the algorithm runs within microseconds. Further work is ongoing prior to the start of the work in this proposal to ensure that the network is still performant and fast when the input and outputs are files in the ATLAS data format, using state-of-the-art computing resources provided by IBM at CERN. 

Tests of autoencoders as anomaly detection algorithms at the LHC are underway by members of the CMS experiment~\cite{CMSAE}, and proof-of-principle tests of autoencoders for compression on a limited number of variables  have been done by LHCb~\cite{ref:lhcb_ae}. 
This proposal aims to test autoencoders for compression on a large scale for the first time, using the largest jet dataset ever recorded using the TLA technique.

In this project, we will profit from the presence of an experienced software engineer during the first two years, to supervise the work of Master’s students augmenting the \textsc{Realdark} team and perform the optimization of the network prior to the large-scale test, and of the postdoctoral researchers who will further test the compression method in a realistic setup on the ATLAS test-bed machines. 

As a byproduct of the data compression work, the existing network will be re-trained, optimized and used in an anomaly detection algorithm to monitor problems in data taking and calibration as in WP2. It will be tested with different levels of tolerance for anomalies using known problematic data, so that it can be calibrated to match what needed to evaluate the performance of analyses in WP3-4. 




\color{red}\textbf{TODO: Reorganization done up to here}\color{black}


