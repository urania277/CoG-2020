%NOTE: This doc formatted according to http://ec.europa.eu/research/participants/data/ref/h2020/call_ptef/pt/2018-2020/h2020-call-pt-erc-stg-2019_en.pdf

\documentclass[11pt,a4paper]{article}
\usepackage[left=2.0cm,top=2.0cm,right=2.0cm,bottom=1.5cm]{geometry}               
%\usepackage[subtle]{savetrees}
\usepackage[bibbreaks=normal, paragraphs=normal, floats=tight, mathspacing=tight, lists=tight, title=normal, margins=normal, wordspacing=normal, tracking=normal, charwidths=normal, bibnotes=normal, mathdisplays=tight, leading=normal, indent=normal, bibliography=tight, sections=tight]{savetrees} 

\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{xspace}     
\usepackage{wrapfig}
\setlength\intextsep{0pt}

\usepackage{color}
\usepackage{colortbl}
\usepackage{amsmath} % Adds a large collection of math symbols                  
\usepackage{ifthen} % for conditional statements               
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{upgreek} % Adds in support for greek letters in roman typeset       
\usepackage{titling}
\usepackage{makecell}
\usepackage{pgfgantt}
\usepackage{lscape}
\usepackage{multirow}
\usepackage{titlesec}
\usepackage{url,booktabs,amsmath,hepunits,abhepexpt,abhep,xcolor}%amsmath
\usepackage[colorlinks]{hyperref}    % Hyperlinks in references
\usepackage[all]{hypcap} % Internal hyperlinks to floats.

\newboolean{uprightparticles}
\setboolean{uprightparticles}{false} %Set to true to get roman particle symbols

%If we need more space we can investigate: https://tex.stackexchange.com/questions/273086/use-smaller-headheight-in-fancyhdr, https://tex.stackexchange.com/questions/271159/turn-off-fancyhdr-auto-spacing
\usepackage{fancyhdr}

\usepackage{rotating}

%\usepackage{fancyheadings}
\pagestyle{fancy}


% Playing with the page size 

\usepackage{array}
\newcolumntype{y}[1]{>{\raggedleft\arraybackslash}p{#1}}

%\renewcommand*{\arraystretch}{1.2}

%TB
%\addtolength{\oddsidemargin}{-10pt}
%\addtolength{\evensidemargin}{-10pt}
%\addtolength{\textwidth}{25pt}
%\addtolength{\textheight}{76pt}
%end TB

%\addtolength{\textfloatsep}{-2pt}
\setlength{\droptitle}{-44mm}

\setlength{\headwidth}{\textwidth}


\newboolean{articletitles}
\setboolean{articletitles}{false}

\usepackage{cite}
\usepackage{mciteplus}

\newboolean{inbibliography}

\titlespacing{\section}{0pt}{4pt}{0pt}
\titlespacing{\subsection}{0pt}{4pt}{0pt}
\titlespacing{\subsubsection}{0pt}{4pt}{0pt}

%\titlespacing{\section}{0pt}{\parskip}{0pt}
%\titlespacing{\subsection}{0pt}{\parskip}{0pt}
%\titlespacing{\subsubsection}{0pt}{\parskip}{0pt}

%Garamond saves space and looks way poncier
%\usepackage[T1]{fontenc}
%\usepackage[urw-garamond]{mathdesign}
\usepackage{ebgaramond}
\renewcommand\labelitemi{$\bullet$}


\usepackage{fontspec}

\setmainfont{EBGaramond-Regular}[
  BoldFont = EBGaramond-Bold,
  ItalicFont = EBGaramond-Italic,
  BoldItalicFont = EBGaramond-BoldItalic]
  
%\usepackage[cmintegrals,cmbraces]{newtxmath}
%\usepackage{ebgaramond-maths}
%\usepackage[T1]{fontenc}

% Helvetica for sans serif
% (scaled to match size of Palatino)
%\usepackage[scaled=0.90]{helvet}
%\usepackage{helvet}

\title{{\Large Part B1 - ERC Consolidator grant}}
\author{{\normalsize Caterina Doglioni}}
\date{}                                           % Activate to display a given date or no date

\usepackage[parfill]{parskip}
%\setlength{\parskip}{0.5em}


\begin{document}
\lhead{{\small Doglioni}}
\chead{{\small Part B1}}
\rhead{{\small REALDARK}}
\begin{center} 

{\Large\bf ERC Consolidator Grant 2020} \\
	{\Large\bf Research Proposal [Part B1]}  \\
 
\vspace{2cm} 
{\huge {\bf }}   \smallskip  

\vspace{2cm} 
{\Huge{REALDARK}} \\ 
\vspace{1cm} 
\vspace{1cm}
\end{center} 
\begin{tabular}{rcl}
Principal Investigator & : & Dr.~Caterina~Doglioni \\
Host Institution & : & Lund University \\ 
Proposal duration in months & : & 60 \\
\end{tabular}  
\vspace{2cm}


\begin{center} {\bf Summary}  \end{center}

The Standard Model of Particle Physics (SM) describes the fundamental particles and interactions in ordinary matter. Despite the SM's success in predicting experimental results, it fails to account for the large excess of unobservable (Dark) matter in the Universe. A compelling hypothesis is that Dark Matter (DM) processes can be created from SM particle collisions such as those produced by the Large Hadron Collider and recorded by the experiments at the CERN laboratory. 
%Tim's comment: "compelling" probably OK in the abstract, but make sure you convince the reader in the main proposal that it is really compelling

In the \textsc{Realdark} project, I will consolidate my leadership in dark matter searches with innovative data-taking techniques, with a team of postdoctoral researchers and students at Lund University working on the ATLAS experiment. 

%Oxana: Expand on real-time analysis and why this is novel (wait for AB's comments to do this)
This research program makes use of real-time data analysis techniques to make the most of the vast amount of LHC data collected by the ATLAS experiment, 
%Oxana doesn't like "resource-constrained" as it's not clear where it's coming from
expanding the ATLAS physics program in a resource-constrained, data-rich environment. 
The technical outcomes of \textsc{Realdark} will be shared with the wider community, providing valuable input in terms of technological advancements. 

%Following was edited from "extend and deploy" following Tim's comment that it sounds like I'm doing the same thing over
Relying on preliminary results achieved within my ERC Starting Grant \textsc{Darkjets}, this proposal delivers new ways of taking data for the upcoming data-taking period, and exploits the datasets recorded to search for evidence of processes related to dark matter at the LHC. 
We will pursue broad yet sensitive searches for dark matter models that can be discovered at the LHC (weakly interacting massive particles and dark sector particles) at a crucial time for the global quest for dark matter. 
The results of this project will provide outputs that will define the direction of future experiments and theoretical efforts. 
The planned searches will yield either a discovery of a dark matter particle candidate ready to study in connection with astrophysical observations, or constraints on dark matter’s particle nature. 

\clearpage

\section*{Section A: Extended synopsis of the proposal} 

\medskip

\section{Aims and impact of this research project} 
\smallskip

The first years of data taking at the Large Hadron Collider (LHC)~\cite{LHC2008} at CERN yielded the discovery of a new fundamental particle, the Higgs boson~\cite{Khachatryan:2016vau}. With this and other notable results, the LHC data has confirmed the predictive power of the Standard Model (SM) of particle physics, the theory of fundamental particles and non-gravitational interactions. However, the amount of ordinary matter described by the SM is exceeded by a factor of five by a kind of unknown matter as determined by cosmological observations, called Dark Matter (DM)~\cite{Bertone:2016nfn}. 

%My big idea is to expand on successful searches for DM that I pioneered to look elsewhere because now it's the time to expand that program. 

Theoretical models that explain the abundance of DM in the universe include massive DM particles that interact weakly with ordinary particles, called WIMPs. These can be produced at the LHC in collisions of ordinary matter (see e.g.~\cite{Boveia:2018yeb} and references therein). 
Creating DM in controlled conditions enables the study of its interactions with ordinary matter, complementing experiments searching for cosmological evidence of DM and astrophysical observations. %~\cite{Boveia:2018yeb}. 
WIMP DM searches have been a flagship of the physics programmes of LHC experiments. In my Starting Grant (StG) I have led novel and comprehensive searches for particles that could reveal SM-DM interactions. I have ensured that the world-leading LHC constraints that resulted had a widespread impact in the global quest for DM, within a consistent theoretical framework that has been adopted by most LHC WIMP searches so far~\cite{Abercrombie:2015wmb}.  

The lack of evidence for WIMPs to date motivates a two-prong approach for DM searches at the LHC. In this Consolidator Grant proposal I will:
\begin{itemize}
    \item advance the state of the art for WIMP searches by enhancing their sensitivity to probe even rarer interactions.%SM-DM 
    \item enable and deliver new searches for DM particle hypotheses beyond the WIMP paradigm. 
    %Tim: overselling, seems like these are only models out there, suggest to move this part to later when motivating the models
    %A subset of these models postulates a new force akin to the strong interaction in the SM that could have so far escaped detection. 
\end{itemize}

The discovery of new, rare processes, at a time when traditional data-taking methods consistently agree with the SM, mandates technical and technological innovation. The LHC collides bunches of protons up to 30 million times per second. Recording and processing all detector data for further analysis is unfeasible: only a small fraction of interesting data can be selected by the experiment’s \textit{trigger systems} due to constraints on both processing and storage; the rest is discarded. This leads to a loss of sensitivity to large areas of parameter space for DM models. 

%Current data-taking methods do not provide sensitivity to large areas of parameter space for DM models. 
In \textsc{Realdark}, my team and I will break the traditional paradigm of first recording the entirety of raw detector data and then analyzing it, by enabling ATLAS data to be reduced and processed in real-time.  
%Oxana does not like "utility"
The searches in this project will be enabled by innovative data-taking techniques at the earliest stage of data selection and processing, increasing the utility of the data recorded by the ATLAS experiment as a whole. 
In ATLAS, these techniques are called Partial Event Building and Trigger Level Analysis (TLA\footnote{Analogue to the techniques of \textit{Data Scouting} in CMS~\cite{Khachatryan:2016ecr} and \textit{Turbo stream} in LHCb~\cite{Aaij:2016rxn}}, whose ATLAS proof-of-principle was delivered by my StG~\cite{Aaboud:2018fzt})~\cite{Aaboud:2016leb}. 
In the PEB technique, only a subset of the ATLAS detector data is written out for later reconstruction, rather than the full detector information. 
In the TLA technique, implemented via PEB, most of the initial data analysis and calibration is performed in real-time ($< ms$) within the trigger system. 
This permits to retain only a small amount of high-level information for further analysis, instead of raw detector data. 
These techniques can reduce the size of the data used for physics analysis by a factor 2--200, 
%5 kB from 1 MB for TLA
overcoming the storage limitations that would otherwise force ATLAS and other LHC experiments to discard the majority of data of interest for many DM searches.
%The combination of the two has never been implemented before. 
%Talk about PEB also?
%, they reduce the size of the data used for physics analysis by a factor of \color{red}X\color{black},  %milliseconds after being taken
%taken, and deliver a dataset with unprecedented sensitivity. 
%From ATLAS trigger paper:
%To reduce event size, some of these streams use partial event building (partial EB), which writes only a predefined subset of the ATLAS detector data per event. For Run 2, events that contain only HLT reconstructed objects, but no ATLAS detector data, can be recorded to a new type of stream. 
The physics results from this project, derived analysis products, and tools for their interpretation will be disseminated to the broader DM community in order to generate  impact beyond particle physics. %generate impact what does this mean? 
%~\cite{Bertone:2018xtm}. 
%\end{itemize}
%	\item by exploiting the unprecedented sensitivity of the data recorded with these techniques by leading a comprehensive set of searches for DM models beyond the WIMP paradigm that . I will lead searches for new DM candidates and associated particles, which until now have escaped detection; % in this first phase of LHC data taking
%	\item by disseminating
The outcomes of this project will have a transformative effect in terms of both data-taking innovations and in the global quest for DM. 
Data storage requirements are a widespread concern for LHC upgrades, as well as for many experiments where the increase in data collection is not matched by a proportional increase in resources~\cite{Alves:2017she,Allen:2018yvz}. 
With my team of two postdocs, two PhD students and a software engineer in \textsc{Realdark}, we will directly address this challenge, and develop solutions that can be ported beyond the LHC at a crucial time for the particle physics and astroparticle communities.  
During the period spanned by this project, 
The update to the European Strategy of particle physics will be adopted by the global HEP community~\cite{Strategy:2019vxc}, and its next iteration will set concrete priorities for future collider projects in light of the results of the next data-taking period of the LHC (\textit{Run-3}).
Moreover, a number of astroparticle and non-collider experiments will start taking data~\cite{APPECStrategy,Beacham:2019nyx}. 
This project delivers as outputs discoveries or constraints that are valuable input to these scientific communities by using Open Science tools, defining the future direction of DM research. 
% within the DM research community,
% Mention that HSF says RTA is crucial for 2020
% Mention that DM people says that they want this stuff done

I am uniquely suited to lead a team to deliver this ambitious and timely research program.  
As evidenced by my CV and track record, my profile combines both technical and scientific proficiency with leadership of large groups of scientists in the DM and technical communities. 
% outputs and development of data-taking analysis tools. 
With my international collaborators and within my StG, I have led the first step towards a paradigm shift in data-taking techniques in ATLAS, from software concept to publication. 
I coordinate synergistic activities towards more efficient data selection and analysis that span all of high energy physics and beyond~\cite{Alves:2017she}. 
I have authored a number of publications on LHC searches for DM and new phenomena, and I have coordinated ATLAS- and LHC-wide working groups instrumental in the design of DM search strategies, such as the Dark Matter Working Group~\cite{DMWGWebsite}, and contributed to the prioritization of future experiments strategies in light of their sensitivity to uncovering the particle nature of DM. 
%~\footnote{Throughout this process, I have been responsible for definining benchmarks, gathering inputs and presenting the case for DM at colliders using various future facilities.}

\section{Advancement to the state of the art from this project} 
\smallskip

The presence of DM does not have an explanation in the SM. 
The absence of a particle explaining DM at the LHC and other experiments indicates that, if DM interacts with SM particles, these interactions must be very feeble and/or the experimental signals of DM must be subtle. 
At the same time, the enormous data rates of modern particle experiments present a challenge: with traditional data-taking methods, it is not possible to record, process and store the large datasets required to reveal DM signals, and the majority of the data has to be discarded milliseconds after being taken. 
  
Two notable examples of discoveries that are impossible with traditional data-taking methods are:
\begin{itemize} 
\item Rare processes where a new particle decays into ordinary matter, mimicked by more frequent SM-only processes.
These new processes are discarded together with the vast amount of irreducible SM background;%mention EW scale here
\item Processes where new particles leave non-standard signals in the detector. 
In these cases, the exact content of the collision is too time-consuming to reconstruct within the timing budget in which a decision to keep an event must be made. Therefore, the features that distinguish signal events from the more frequent background events are missing and the event is discarded.
\end{itemize}

These examples map to two classes of models that explain the particle nature of dark matter, both able to reproduce the amount of DM measured in the universe (relic density). 

\begin{wrapfigure}{R}{0.5\textwidth} 
\begin{center}
\includegraphics[width=0.45\textwidth]{figs/SensitivityWIMP.png}
\caption{Sketch of sensitivity of WIMP searches in this project. \scriptsize \color{red} Make this sketch into a plot with the expected sensitivity of the searches. \color{black}\label{fig:pastFutureConstraints} }
\end{center}
\end{wrapfigure}

The first class of models corresponds to the current state of the art for LHC searches, also as a result of my StG~\cite{Abercrombie:2015wmb,Boveia:2018yeb}. 
In these models, DM is a massive particle that interacts only weakly with SM particles -- a WIMP. 
Due to their weak interactions with the detector material, WIMPs are traditionally sought at collider experiments by selecting events with an energy imbalance where an invisible particle has escaped detection. 
WIMPs can be produced from LHC proton-proton collisions through a new massive particle mediating the SM-DM interaction, analogously to the W and Z particles mediating the weak force. 
In addition to decaying into DM particles, this mediator will also decay into ordinary matter, through the same interactions responsible for its production. 
In my StG, I delivered a new set of searches where the DM mediator decays to two quarks, leading to two sprays of collimated particles (jets) in the detector. 
%TODO: add here why I care about jets and EW scale
Using traditional techniques, signal events with masses around the electroweak scale are discarded together with high-rate backgrounds from the strong force (Quantum Chromodynamics, or QCD), to meet storage constraints.
The search in the mediator mass range 450 GeV -- 1 TeV was made possible by the application of the TLA technique to jets,
while the range 250 -- 450 GeV was covered by a new signature that my colleagues and I introduced to ATLAS, as described below. 
These searches used the LHC dataset recorded from 2015 to 2018 (called \textit{Run-2}), and have set the most stringent constraints to date for DM mediators with masses between 250 GeV and 1 TeV (see shaded \color{red} color \color{black} areas in Fig.~\ref{fig:pastFutureConstraints}). 
%Mention why TLA is good here
In this project, my CoG team and I will significantly extend this technique by enabling the use of the TLA technique for photons, electrons and muons for the first time, in order to gain sensitivity to a larger range of mediator masses as well as to new dark matter models. 
We~\footnote{When the term "we" is used, it refers to myself as PI and to the personnel hired within \textsc{Realdark}.} will validate these data-taking techniques and hte new ATLAS trigger as a whole with physics searches and measurements in early LHC data. 
We will subsequently exploit the full dataset containing TLA photons and jets for more sensitive and lower-mass WIMP mediator searches (see shaded \color{red} color \color{black} areas in Fig.~\ref{fig:pastFutureConstraints}). 
%Change wording to WAF/VR

Motivated by constraints set on WIMPs, the second class of models postulates interactions that are much feebler, with much lighter mediators. 
These models~\cite{Strassler:2006im,Cohen:2017pzm} predict a multitude of new particles in addition to the DM candidate, mirroring the complexity of the SM in theories similar to the strong force (\textit{dark QCD}). 
An unambiguous signal of these models are \textit{dark jets} which, in addition to visible particles, are comprised of invisible particles and light dark matter mediators which decay into low-energy electrons and muons~\cite{Curtin:2014cca}. 
Standard searches in two-jet final states are not optimally sensitive to these scenarios. %, since they escape detection and are discarded at the trigger level. 
This is because of the very large SM QCD backgrounds (a problem shared by WIMP mediator searches), and because the HLT computing farm is unable to build and identify the characteristic features of dark QCD jets in time and signal events are considered noise. 
%shorten this sentence above
To solve this problem, we will combine the TLA and the Partial Event Building technique for the first time in ATLAS, after extending TLA to electrons and muons. %too much???
By augmenting the limited trigger level information with the full set of raw data but only in the region of interest behind the dark jets, we will overcome storage and processing limitations, and enable the reconstruction of features that distinguish signal from background at a later processing stage where more resources are available. 
We will use the dataset recorded with this technique to map the parameter space of dark QCD theories and discover or set stringent constraints on models never tested at the LHC. 
The unique dataset recorded with this new technique will be made available for other searches and measurements, extending the potential for discovery of the entire ATLAS physics search programme. 

It is crucial that the results obtained in these searches are put into the broader context of the global search for dark matter. %ok but why
For this reason, I will continue leading the way to define community standards for LHC DM searches, so that any LHC discovery or constraint can be considered %exploited??
in synergy with complementary astroparticle, non-collider and cosmological measurements. 
Building on the success of the Dark Matter Working Group that under my leadership set the standards for LHC WIMP search targets and for the presentation of results, I will lead initiatives that bring together the flourishing experimental and theoretical research communities studying non-WIMP DM scenarios~\cite{iDMEu}. 

%the impact of this proposal will not be limited to high energy physics but will be disseminated further afield to generate impact in DM direct detection and cosmological disciplines.  
% capitalizing on the information from many experiments coming online in the next decade

\section{Project organization and description} 
\smallskip


\begin{wrapfigure}{L}{0.6\textwidth} 
%\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.6\textwidth]{figs/WPs_shorter}
\caption{\label{fig:WPs} \footnotesize Schema of work packages and expected results.
%\color{red} This will become an in-line figure, possibly on the 1st page, after feedback. If needs be, the first section will be shortened. \color{black}
}
\end{center}

\vskip10pt
\end{wrapfigure}
%\end{figure}

The project consists of five logically interconnected work packages.
%to maximise impact. 
The work packages and their interconnections are indicated in Fig.~\ref{fig:WPs}. 
In \textbf{WP1} we will overcome the technological constraints that limit the sensitivity to a variety of physics phenomena including DM with the use of non-standard data-taking and recording techniques, and employ machine learning techniques for compressing data towards further gains in event storage.  
In \textbf{WP2}, we will lead a programme of searches and measurements using the first LHC Run 3 data and in so doing commission the techniques developed in WP1. 
%Oxana thinks that world-best and world-first is bragging excessively
In \textbf{WP3 and WP4}, we will use the dataset recorded with the fully commissioned techniques developed in WP1 and WP2 to perform world-best and world-first searches for current and state-of-the-art dark matter models. 
In~\textbf{WP5}, we will interpret the results of those searches in coherent frameworks that go beyond model boundaries with the inclusion of input from LHC measurements and non-collider DM searches using Open Science tools, and work in synergy with the broader community for optimal contextualization and dissemination of results and tools. %transgress??? Conor wants this word I don't 

%\begin{description}

%\item[WP1] In WP1 I will overcome the technological constraints that limit the sensitivity to a variety of physics phenomena including DM, with the use of non-standard data-taking and %recording techniques, and employ machine learning techniques for forward-looking improvements. 
%\item[WP2] In WP2, I will lead a programme of searches and measurements using the first LHC Run 3 data and in so doing commission the techniques developed in WP1. 
%\item[WP3, WP4] In WP3 and WP4, I will use the dataset recorded with the fully commissioned techniques developed in WP1 and WP2 to perform world-best and world-first searches for current and state-of-the-art dark matter models. 
%\item[WP5] In~\textbf{WP5}, I will interpret the results of those searches in coherent frameworks that transgress model boundaries with the inclusion of input from LHC measurements and non-collider DM searches using Open Science tools, and work in synergy with the broader community for optimal dissemination of results and tools. 
%\medskip

%\end{description}

\subsection*{\color{teal} $\blacksquare$ \color{black} WP1: Advancing real-time analysis in ATLAS}

\begin{wrapfigure}{R}{0.4\textwidth} \includegraphics[width=0.4\textwidth]{figs/TLAPEB}
\caption{\label{fig:TLAPEB} \small Fraction of event rates recorded and trigger storage used for standard and TLA technique. 
The event rate recorded with TLA is much higher than the rate of events for standard data taking, for a much smaller amount of storage resources, as measured in 2017 ATLAS data-taking~\cite{ATLASComputing}. \scriptsize }
\end{wrapfigure}
%\color{red} If available, include estimates for this proposal. If not, change colors and only include Standard data taking and Trigger Level Analysis. Also, show how much this could be compressed if compression worked. \color{black}}

%Oxana wants this in the first page
As the main technological advancement delivered by this project, we will deploy and commission the two techniques of TLA and Partial Event Building. 
As a pioneer of real-time analysis, I have delivered proof-of-concept studies for these techniques, but they must now be developed and commissioned on a much larger scale in WP1 in order to exploit them for groundbreaking DM searches with the Run-3 dataset.

\textbf{TLA}-recorded events only contain high level information reconstructed in the trigger, and they are considerably smaller than events with full detector data. 
This technique therefore enables the recording of a much larger number of events within the same data volume, as shown in Fig.~\ref{fig:TLAPEB}.  
With my StG team, I have shown that this technique is effective in a proof-of-concept use case of dark matter mediators~\cite{Aaboud:2018fzt}. 
Within this project I intend to broaden this paradigm to photons, electrons and muons so that it can be used to extend the ATLAS experiment reach to a much larger set of searches and measurements. 

In the \textbf{Partial Event Building} technique, the information available to real-time analyses is augmented by a subset of the raw detector information in selected regions of the detector. This paradigm combines the data rate enhancements of real-time analysis with the added precision of full offline reconstruction in a user-configurable manner. 
Non-ordinary features signaling the presence of new phenomena can then be detected \textit{a posteriori}. 
This project will commission and use this technique for physics searches for the first time in ATLAS. 

Both of these techniques reduce data storage requirements, but they can be complemented by \textbf{further gains in data compression}. 
In this project, I will pursue a method for data compression using using autoencoder deep neural networks~\cite{Autoencoders}. %machine learning (ML) algorithms. 
My students and I have obtained preliminary results in collaboration with other ATLAS ML and computing experts.
These results show showing the potential for an additional compression factor of two with minimal performance loss. 
This is a forward-looking activity targeting the high-luminosity LHC run starting in 2026 as well as experiments beyond the LHC.

\subsection*{\color{green} $\blacksquare$ \color{black} WP2: Commissioning the Run-3 trigger system with physics}

During the shutdown between the Run-2 and Run-3 LHC data-taking periods, the ATLAS experiment trigger system is undergoing significant upgrades~\cite{Aad:1602235}, and the trigger and reconstruction software is being rewritten~\cite{Bielski:2674286}. %Stewart:2016pay
Thorough commissioning and testing is mandatory, first using simulation and cosmic ray data, and subsequently with LHC commissioning data. 
In WP2, \textbf{together with my team, I will test a completely new software trigger framework and its performance via early measurements and searches}, using the already-established TLA techniques from Run-2~\cite{Aaboud:2018fzt}. This work will be documented by physics and technical publications.  

We will select events where at least two jets, muons or electrons are detected and reconstructed in real-time within the trigger system, and compare the performance of reconstruction and calibration between trigger-level and traditional data-taking approaches. 
%Improving the trigger-level reconstruction makes ATLAS data-taking more efficient as a whole. 
Beyond the technical publications documenting the new software framework and its commissioning with LHC data, this WP will enable physics publications of the first Run-3 searches for low-mass DM mediators. 
We will make extensive use of the Open Science tool RECAST~\cite{Schuy:2019awp} to preserve the end-to-end analysis workflow, from detector data to final plots so that we can take advantage of this work for the searches in WP3.  

\subsubsection*{\color{yellow} $\blacksquare$ \color{black} WP3: WIMP dark matter mediator searches}

In WP3, \textbf{my team and I will use the TLA technique to search for decays of WIMP mediators in a scenario that is not fully constrained by existing searches}.
This search targets mediators with masses between 150 and 350 GeV, a mass range close to where the SM massive force mediators reside that is still poorly constrained, as neither traditional searches nor jet-only TLA searches have optimal sensitivity (see Fig.~\ref{fig:pastFutureConstraints}).  

We will employ the detector signature that I introduced to ATLAS during Run-2 using traditional data-taking methods, where the mediator is produced in association with an additional jet or photon~\cite{Aaboud:2019zxd}. 
This approach yields the world-leading constraint in the upper mass range of this region. 
\textbf{Performing this search with the TLA method will significantly increase its discovery potential}, 
allowing for an order of magnitude more signal events to be recorded.% and a factor \color{red}M \color{black} improvement in signal-to-background ratio. 

During the 2022-2025 grant period, we will also analyse data in the two-jet signature to search for mediators with masses above 350 GeV with the entire Run-3 dataset. 
This analysis will capitalize on the end-to-end analysis workflows set up in WP2 that make the analysis more efficient and ensure that the Run-3 results can be combined with the full Run-2 results and used to constrain a wide set of physics models~\cite{Kim:2019rhy}.   

\subsubsection*{\color{orange} $\blacksquare$ \color{black} WP4: Dark sector searches}

While the searches in WP2 are powerful probes of WIMPs, they are not sensitive to DM mediators whose interaction with the SM is even feebler, or to GeV-scale DM mediators. 
For this reason, in WP4 \textbf
{my team and I will deploy the combination of TLA and Partial Event Building developed in WP1 to discover signs of DM in a different class of models yielding new detector signatures}. 
This project focuses on two new searches.  

Firstly, we will search for evidence of models where the DM candidate particles are produced in association with a large number of other dark sector particles. 
Since these dark sector particles also interact through the SM's strong force, while the DM particles escape detection, this leads to \textit{semi-visible dark jets}~\cite{Cohen:2017pzm}.
Prototypes of these searches are being developed with Run-2 data, but they are limited to very high masses and to a narrow range of parameters of the model. 
In this project, we will scan a much larger part of the parameter space with the unique sensitivity enabled by this dataset, focusing on the region where the model can explain the relic density of DM~\cite{Bernreuther:2019pfb}. 

The second search builds on the semi-visible jet results and exploits the availability of electrons and muons at the trigger level enabled by WP1, targeting models where low-energy leptons from the decays of a light dark mediator or a dark Higgs boson are also found within dark jets~\cite{Curtin:2014cca,Falkowski:2010gv}. %Cite Prestel?
Current searches are restricted to higher-energy leptons, and this dataset will extend the sensitivity to low-mass, low-energy and exotic objects overlooked in searches so far. 

\textbf{REWRITE THIS COHERENTLY}
\subsubsection*{\color{violet} $\blacksquare$ \color{black} WP5: Dissemination, communication, synergies}

WP5 defines frameworks and optimal search regions in the context of the global DM search community, resulting in enhanced conceptualization and dissemination of tools and results.
This dedicated WP ensures that the results of the project have a broad research impact that exploits both local synergies and synergies with other communities. 

Within WP5, my team and I will pinpoint the optimal parameter space to be targeted for the new searches in WP4, and adapt the selection of events to be stored in the dataset accordingly. 
%event generator = pp collision modelling?
This will be delivered in collaboration with the local Lund theory group, who are authors of the most widely used implementation of the theory in the proton-proton collision modelling software Pythia ~\cite{Sjostrand:2007gs}. 
We will use software that enables the use of precision results released by LHC collaborations~\cite{Butterworth:2016sqg}, using them for the first time to constrain dark QCD models. 
In WP5 we will ensure that all physics results from WP2-4 are disseminated and implemented in Open Science tools (e.g. RECAST, HEPData) that meet the needs of other communities in terms of reproducibility, usability and complementarity in compliance with the FAIR principles~\textbf{CiteMe}. 
The successful experience of the Dark Matter Working Group for the LHC DM community is a stepping stone for a new, ambitious initiative that is being brought to the attention of the community~\cite{iDMEu}. 
This initiative will enable results from this project to reach a much broader context, including non-collider experiments, astrophysics, cosmology and multimessenger astronomy. 
An innovative aspect of this project is that its dissemination strategy is not limited to physics results but includes algorithms and tools. 
As I and many others advocated during the process leading to the update of the European Strategy of Particle Physics~\cite{Doglioni:2019fza}, challenges related to data acquisition, selection and analysis can be tackled more effectively and efficiently by going beyond a single experiment's boundaries. 
%and in the software catalogue of the European ESCAPE project, 
%Oxana doesn't like computing resources
For this reason, WP5 also includes sharing analysis workflows within a collaborative Dark Matter virtual environment that I will co-lead, envisaged by the Europe-wide particle and astroparticle ESCAPE project\textbf{CiteMe}, as well as data compression algorithms with other experiments where a reduced storage footprint is necessary to increase the physics potential within the same computing resources. %, such as gravitational wave experiments \color{red}~\cite{GravWavesCompression}\color{black}. 

\section{Timeliness and timeline of the research program} 
\smallskip

I will lead a research team of two postdoctoral researchers and two graduate students, working on the two lines of physics analysis in this project, and frequently collaborating to share technical tasks and software. An additional software engineer will augment the team during the demanding time of the LHC Run-3 startup. 
For WP1, the team will be joined by talented Lund University undergraduates that I have a track record of recruiting and training who will work on the forward-looking ML compression activities.
The research program spans the entire upcoming Run-3 LHC data-taking period.
%and its timeline is shown in Fig.~\ref{fig:timeline}. 
The 2021-2026 period is the ideal time to advance the state-of-the-art in processing of large datasets and DM searches, as it ensures continued impact through a significant extension of my current successful StG research program that pioneered proof-of-principle real-time searches in ATLAS. 
The current LHC schedule includes an initial commissioning period where innovations can be deployed and tested via early measurements with Run-3 start-up data, which forms the first part of this project (WP1 and WP2), readying them for the second phase of the project where the LHC will be in production mode.   
The second (WP3 and WP4) phase of this project will exploit the wealth of LHC data delivered by WP1 and WP2 for novel DM searches, with a dataset that will be more than \color{red}twice \color{black} as sensitive to the physics observables of this project than the data collected so far. 
Throughout the timeline of entire project, in WP5 we will share tools and results within the global DM community, to answer one of the most pressing questions of our universe with a discovery or by defining future search directions in which such a discovery will be made. 

%contextualize the results of the searches in this proposal and the LHC research program within
%The team will start with commissioning the techniques in WP1 (2021) as the the students fulfill their authorship qualification tasks in the trigger system. 
%The jet and photon TLA techniques will be ready in coincidence with early data taking in Summer 2021, while the delivery of TLA muons and electrons is planned for mid-2022. 
%Partial Event Building will also be commissioned with 2022 data, and the second

% Mention who asked you to do this rather than saying who you're doing this with
%In addition to further enhancing my standing as an expert in DM, 

%As a recognized expert in DM searches and collider data taking techniques with a track record of synergistic interdisciplinary activities, the impact of this proposal will not be limited to high energy physics but will be 
\clearpage

\setboolean{inbibliography}{true}
\bibliographystyle{JHEP}
\bibliography{researchrefs}

{\bf Note:} The PI is the editor of Refs.~\cite{Boveia:2018yeb,Abercrombie:2015wmb,Aaboud:2018fzt,Aaboud:2019zxd,Doglioni:2019fza}, the initiator of the activities in Refs.~\cite{DMWGWebsite,iDMEu} and has edited sections of Refs.~\cite{Strategy:2019vxc,Alves:2017she,Aaboud:2016leb}. The PI is the author of all publications by the ATLAS collaboration.
% -- Refs.~\cite{Alves:2008zz,CERN-LHCC-2014-016,LHCb:2011aa,Aaij:2013mga,Aaij:2014ywt}.
%The PI is the contact author within LHCb for Ref.~\cite{Aaij:2014ywt}.

\end{document}